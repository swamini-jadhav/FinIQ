# -*- coding: utf-8 -*-
"""FINQ_FinBERT model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13lmeVrvxvuJhsbcTw8zv6xr8h_6VmjxW
"""

print("ğŸ“¦ Installing required packages...") #installing imp packages  MY TRAINED MODEL IS SAVED in-  /content/drive/MyDrive/FinIQ_Model
print("â±ï¸  This will take 2-3 minutes\n")

!pip install -q transformers datasets torch scikit-learn

print("\nâœ… All packages installed successfully!")
print("ğŸ‰ Ready to proceed!")

import pandas as pd #preparing the dataset
from datasets import Dataset
from sklearn.model_selection import train_test_split
import os

print("ğŸ“‚ Loading your CSV file...")
df = pd.read_csv('all-data.csv', encoding='latin-1', names=['sentiment', 'sentence'])

print(f"âœ… Loaded {len(df)} examples")
print(f"\nFirst few rows:")
print(df.head())

# Clean the data
print("\nğŸ§¹ Cleaning data...")
sentiment_map = {
    'positive': 2,
    'neutral': 1,
    'negative': 0
}

df['label'] = df['sentiment'].map(sentiment_map)
df = df[['sentence', 'label']].dropna()
df = df.drop_duplicates(subset=['sentence'])

print(f"âœ… Cleaned data: {len(df)} examples")
print("\nLabel distribution:")
print(df['label'].value_counts().sort_index())

# Split into train/test
print("\nâœ‚ï¸ Splitting data...")
train_df, test_df = train_test_split(
    df,
    test_size=0.2,
    random_state=42,
    stratify=df['label']
)

# Convert to HuggingFace Dataset format
train_dataset = Dataset.from_pandas(train_df[['sentence', 'label']], preserve_index=False)
test_dataset = Dataset.from_pandas(test_df[['sentence', 'label']], preserve_index=False)

print(f"âœ… Training set: {len(train_dataset)} examples")
print(f"âœ… Testing set: {len(test_dataset)} examples")

# Save datasets
os.makedirs('./data', exist_ok=True)
train_dataset.save_to_disk('./data/train')
test_dataset.save_to_disk('./data/test')

print("\nğŸ’¾ Datasets saved to ./data/")
print("ğŸ‰ Dataset preparation complete!")

from google.colab import drive

print("ğŸ“ Mounting Google Drive...")
print("âš ï¸  A link will appear - click it and authorize\n")

drive.mount('/content/drive')

print("\nâœ… Google Drive mounted successfully!")
print("ğŸ“‚ Your Drive is accessible at: /content/drive/MyDrive")

# Create save directory
import os
save_path = '/content/drive/MyDrive/FinIQ_Model'
os.makedirs(save_path, exist_ok=True)

print(f"âœ… Model save directory created: {save_path}")
print("â­ Your model will be permanently saved here!")

from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments
from datasets import load_from_disk
import numpy as np
from sklearn.metrics import accuracy_score, precision_recall_fscore_support
import torch
import gc
import os

# Save path in Google Drive
save_path = '/content/drive/MyDrive/FinIQ_Model'

print("ğŸš€ STARTING MODEL TRAINING")
print("=" * 70)
print("â±ï¸  Expected time: 15-20 minutes")
print("â˜• Grab a coffee and relax!")
print("=" * 70)

# Clear memory
torch.cuda.empty_cache()
gc.collect()

# Load datasets
print("\nğŸ“‚ Loading datasets...")
train_dataset = load_from_disk('./data/train')
test_dataset = load_from_disk('./data/test')

# Use 2000 examples for balanced speed + accuracy
print("âš¡ Using 2000 training examples (balanced approach)...")
train_dataset = train_dataset.select(range(min(2000, len(train_dataset))))
test_dataset = test_dataset.select(range(min(400, len(test_dataset))))

print(f"âœ… Training: {len(train_dataset)} examples")
print(f"âœ… Testing: {len(test_dataset)} examples")

# Load tokenizer
print("\nğŸ”¤ Loading FinBERT tokenizer...")
tokenizer = AutoTokenizer.from_pretrained("ProsusAI/finbert")
print("âœ… Tokenizer loaded!")

# Tokenize data
print("\nâš™ï¸ Tokenizing data...")
def tokenize_function(examples):
    return tokenizer(
        examples["sentence"],
        padding="max_length",
        truncation=True,
        max_length=128
    )

tokenized_train = train_dataset.map(tokenize_function, batched=True)
tokenized_test = test_dataset.map(tokenize_function, batched=True)
print("âœ… Tokenization complete!")

# Load model
print("\nğŸ¤– Loading FinBERT model...")
from transformers import BertConfig

config = BertConfig.from_pretrained("ProsusAI/finbert")
config.num_labels = 3
config.hidden_dropout_prob = 0.1
config.attention_probs_dropout_prob = 0.1

model = AutoModelForSequenceClassification.from_pretrained(
    "ProsusAI/finbert",
    config=config,
    ignore_mismatched_sizes=True
)
print("âœ… Model loaded!")

# Define metrics
def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    predictions = np.argmax(predictions, axis=1)

    accuracy = accuracy_score(labels, predictions)
    precision, recall, f1, _ = precision_recall_fscore_support(
        labels, predictions, average='weighted', zero_division=0
    )

    return {
        'accuracy': accuracy,
        'f1': f1,
        'precision': precision,
        'recall': recall
    }

# Training configuration
print("\nâš™ï¸ Setting up training configuration...")
training_args = TrainingArguments(
    output_dir="/content/results",
    num_train_epochs=2,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=32,
    learning_rate=2e-5,
    weight_decay=0.01,
    eval_strategy="epoch",
    save_strategy="epoch",
    load_best_model_at_end=True,
    metric_for_best_model="accuracy",
    logging_steps=25,
    save_total_limit=1,
    report_to="none",
    fp16=True,
)

# Create Trainer
print("\nğŸ‘¨â€ğŸ« Creating trainer...")
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_train,
    eval_dataset=tokenized_test,
    compute_metrics=compute_metrics,
)
print("âœ… Trainer ready!")

# START TRAINING
print("\n" + "=" * 70)
print("ğŸš€ TRAINING STARTED!")
print("=" * 70)

try:
    trainer.train()

    print("\n" + "=" * 70)
    print("ğŸ‰ TRAINING COMPLETE!")
    print("=" * 70)

    # Save to Google Drive
    print(f"\nğŸ’¾ Saving model to Google Drive...")
    print(f"ğŸ“ Location: {save_path}")

    trainer.save_model(save_path)
    tokenizer.save_pretrained(save_path)

    # Verify save
    if os.path.exists(save_path):
        files = os.listdir(save_path)
        print(f"\nâœ… SUCCESS! Model saved to Google Drive!")
        print(f"ğŸ“ Total files saved: {len(files)}")
        print(f"â­ Your model is PERMANENTLY saved!")
        print(f"â­ Location: {save_path}")

    # Evaluate
    print("\nğŸ“Š FINAL RESULTS:")
    print("=" * 70)
    eval_results = trainer.evaluate()

    for key, value in eval_results.items():
        print(f"{key}: {value:.4f}")

    print("\n" + "=" * 70)
    print("ğŸ‰ ALL DONE! YOUR MODEL IS READY!")
    print("=" * 70)
    print(f"ğŸ“‚ Model saved at: {save_path}")
    print("ğŸš€ Next step: Test your model!")
    print("=" * 70)

except Exception as e:
    print(f"\nâŒ Error during training: {e}")
    import traceback
    traceback.print_exc()

from transformers import pipeline

print("ğŸ“‚ Loading your trained model from Google Drive...")
model_path = '/content/drive/MyDrive/FinIQ_Model'

sentiment_analyzer = pipeline(
    "sentiment-analysis",
    model=model_path,
    tokenizer=model_path
)

print("âœ… Model loaded successfully!\n")

# Test headlines
test_headlines = [
    "Apple reports record quarterly revenue beating expectations",
    "Tesla stock crashes 15% amid production delays",
    "Microsoft announces stable quarterly earnings",
    "Amazon plans massive expansion with 50,000 new jobs",
    "Bank faces investigation over fraudulent practices",
    "Company maintains dividend despite volatility"
]

print("ğŸ§ª TESTING YOUR AI MODEL:\n")
print("=" * 80)

for i, headline in enumerate(test_headlines, 1):
    result = sentiment_analyzer(headline)[0]

    # Updated label_map to match the string labels returned by the pipeline
    label_map = {
        'negative': ('NEGATIVE ğŸ“‰', 'SELL ğŸ”´'),
        'neutral': ('NEUTRAL â–', 'HOLD ğŸŸ¡'),
        'positive': ('POSITIVE ğŸ“ˆ', 'BUY ğŸŸ¢')
    }

    sentiment, signal = label_map[result['label']]
    confidence = result['score'] * 100

    print(f"\n{i}. ğŸ“° {headline}")
    print(f"   Sentiment: {sentiment}")
    print(f"   Signal: {signal}")
    print(f"   Confidence: {confidence:.1f}%")
    print("   " + "-" * 70)

print("\n" + "=" * 80)
print("ğŸ‰ Your AI-powered sentiment analyzer is working!")
print("=" * 80)